{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "id": "4euARiTbimO6"
   },
   "outputs": [],
   "source": [
    "# libraries \n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import cvxpy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "import time\n",
    "import scipy\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy.sparse import random\n",
    "from scipy import stats\n",
    "import torch\n",
    "import sklearn.linear_model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import scipy.io as sio\n",
    "import torch.nn.functional as F\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (49000, 3, 32, 32)\n",
      "y_train: (49000,)\n",
      "X_val: (1000, 3, 32, 32)\n",
      "y_val: (1000,)\n",
      "X_test: (1000, 3, 32, 32)\n",
      "y_test: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_CIFAR10_data\n",
    "# Load the (preprocessed) CIFAR-10 data.\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "    print(f\"{k}: {v.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "id": "qyXYpGw6imPE"
   },
   "outputs": [],
   "source": [
    "def generate_sign_patterns(X, P, verbose=False): \n",
    "    \"\"\" \n",
    "    Inputs: X (input to the layer), P (number of samples, also number of neurons)\n",
    "    Outputs: P, [D_1,...D_P], [u_1,...,u_P]\n",
    "             where each D is Diag(1[X@u_i>=0]), u_i a random vector\n",
    "    See remark 3.3 in (Pilanci&Ergen 2020)\n",
    "    \"The convex program (8) can be approximated by sampling a set\n",
    "     of diagonal matrices D_1,...,D_P\"\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    sign_pattern_list = []  # sign patterns, stores D_1,...,D_P\n",
    "    u_vector_list = []      # random vectors used to generate the sign patterns\n",
    "\n",
    "    for i in range(P): \n",
    "        # Obtain a sign pattern\n",
    "        # Future work: use a different sampling distribution than Gaussian\n",
    "        \n",
    "        # First, sample u from normal distribution\n",
    "        u = np.random.normal(0, 1, (d,1)) \n",
    "        \n",
    "        # Our sign pattern is Diag(1[X@u_i>=0])\n",
    "        sampled_sign_pattern = (np.matmul(X, u) >= 0)[:,0]\n",
    "        \n",
    "        sign_pattern_list.append(sampled_sign_pattern)\n",
    "        u_vector_list.append(u)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Number of sign patterns generated: \" + str(len(sign_pattern_list)))\n",
    "    return len(sign_pattern_list),sign_pattern_list, u_vector_list\n",
    "\n",
    "class PrepareData(Dataset):\n",
    "    #  converts data from numpy array to torch tensor\n",
    "    def __init__(self, X, y):\n",
    "        if not torch.is_tensor(X):\n",
    "            self.X = torch.from_numpy(X)\n",
    "        if not torch.is_tensor(y):\n",
    "            self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [],
    "id": "BpKsLvrZimPG"
   },
   "outputs": [],
   "source": [
    "# functions for solving the convex problem\n",
    "# tutorial link: https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html\n",
    "class custom_cvx_layer(torch.nn.Module):\n",
    "    def __init__(self, n, d, num_neurons, u_vector_list):\n",
    "        \"\"\"\n",
    "        Variables:\n",
    "        w, v: matrices, arguments to the argmin of (8) in paper\n",
    "        u_vectors_as_np: numpy array of vectors used to generate diagonals\n",
    "        h: torch tensor of u_vectors_as_np\n",
    "        \"\"\"\n",
    "        super(custom_cvx_layer, self).__init__()\n",
    "        \n",
    "        self.w = torch.nn.Parameter(data=torch.zeros(d, num_neurons), requires_grad=True)\n",
    "        self.v = torch.nn.Parameter(data=torch.zeros(d, num_neurons), requires_grad=True)\n",
    "        \n",
    "        u_vectors_as_np = np.asarray(u_vector_list).reshape((num_neurons, d)).T\n",
    "        self.h = torch.nn.Parameter(data=torch.Tensor(u_vectors_as_np), requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Generates sign patterns on the fly, then carries out the prediction\n",
    "        inside of (8)\n",
    "        \"\"\"\n",
    "        print('x shape', x.shape)\n",
    "        print('v shape', self.v.shape)\n",
    "        print('w shape', self.w.shape)\n",
    "        print('h shape', self.h.shape)\n",
    "        \n",
    "        sign_patterns = (torch.matmul(x, self.h) >= 0)\n",
    "        Xv_w = torch.matmul(x, self.v - self.w)\n",
    "        DXv_w = torch.mul(sign_patterns, Xv_w)\n",
    "        y_pred = torch.sum(DXv_w, dim=1, keepdim=True)\n",
    "        \n",
    "        print('raw DXv_w shape', DXv_w.shape)\n",
    "        print('yhat shape', y_pred.shape)\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "def get_nonconvex_cost(y, model, _x, beta):\n",
    "    \"\"\"\n",
    "    Where does this come from? It doesn't look like (2)\n",
    "    \"\"\"\n",
    "    Xv = torch.matmul(_x, model.v) \n",
    "    Xw = torch.matmul(_x, model.w)\n",
    "    Xv_relu = torch.max(Xv, torch.Tensor([0]))\n",
    "    Xw_relu = torch.max(Xw, torch.Tensor([0]))\n",
    "    \n",
    "    prediction_w_relu = torch.sum(Xv_relu - Xw_relu, dim=1, keepdim=True)\n",
    "    prediction_cost = 0.5 * torch.norm(prediction_w_relu - y)**2/y.shape[0]\n",
    "    \n",
    "    regularization_cost = (beta/2) * (torch.sum(torch.norm(model.v, dim=0)) + torch.sum(torch.norm(model.w, dim=0)))\n",
    "    regularization_cost = 2 * regularization_cost # norm for u_j and alpha_j are the same, so just multiply by 2\n",
    "    \n",
    "    return prediction_cost + regularization_cost\n",
    "\n",
    "\n",
    "def loss_func_cvxproblem(yhat, y, model, _x, beta, rho):\n",
    "    \"\"\"\n",
    "    This implements the conex problem described by equation (8)\n",
    "    \"\"\"\n",
    "    # term 1, the square-distance loss\n",
    "    loss = 0.5 * torch.norm(yhat - y)**2/y.shape[0]\n",
    "    \n",
    "    # term 2, the regularization term\n",
    "    m1=torch.mean(torch.norm(model.v, dim=0))\n",
    "    m2=torch.mean(torch.norm(model.w, dim=0))\n",
    "    norms1=torch.norm(model.v, dim=0)#*(torch.norm(model.v, dim=0)>=m1)\n",
    "    norms2=torch.norm(model.w, dim=0)#*(torch.norm(model.w, dim=0)>=m2)\n",
    "    loss = loss + beta * torch.sum(norms1)\n",
    "    loss = loss + beta * torch.sum(norms2)\n",
    "    \n",
    "    # term 3, enforces the inequality constraints\n",
    "    if rho>0:\n",
    "        sign_patterns = (torch.matmul(_x, model.h) >= 0)\n",
    "\n",
    "        Xv = torch.matmul(_x, model.v)\n",
    "        DXv = torch.mul(sign_patterns, Xv)\n",
    "        relu_term_v = torch.max(-2*DXv + Xv, torch.Tensor([0]))\n",
    "        loss = loss + rho * torch.sum(relu_term_v)\n",
    "\n",
    "        Xw = torch.matmul(_x, model.w)\n",
    "        DXw = torch.mul(sign_patterns, Xw)\n",
    "        relu_term_w = torch.max(-2*DXw + Xw, torch.Tensor([0]))\n",
    "        loss = loss + rho * torch.sum(relu_term_w)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def validation_cvxproblem(model, testloader, beta, rho):\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_noncvx_cost = 0\n",
    "\n",
    "    for ix, (_x, _y) in enumerate(testloader):\n",
    "        _x = Variable(_x).float()\n",
    "        _y = Variable(_y).float()\n",
    "\n",
    "        yhat = model(_x).float()\n",
    "\n",
    "        loss = loss_func_cvxproblem(yhat, _y, model, _x, beta, rho)\n",
    "        test_loss += loss.item()\n",
    "        test_correct += torch.eq(torch.sign(yhat), _y).float().sum()\n",
    "        \n",
    "        test_noncvx_cost += get_nonconvex_cost(_y, model, _x, beta)\n",
    "\n",
    "    return test_loss, test_correct, test_noncvx_cost\n",
    "\n",
    "\n",
    "def sgd_solver_cvxproblem(A_train, y_train, A_test, y_test, num_epochs, num_neurons, beta, \n",
    "                       learning_rate, batch_size, rho, u_vector_list, solver_type, LBFGS_param, verbose=False):\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    n, d = A_train.shape\n",
    "    \n",
    "    # create the model\n",
    "    model = custom_cvx_layer(n, d, num_neurons, u_vector_list).to(device)\n",
    "    \n",
    "    if solver_type == \"sgd\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif solver_type == \"LBFGS\":\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(), history_size=LBFGS_param[0], max_iter=LBFGS_param[1])#,\n",
    "    \n",
    "    # arrays for saving the loss and accuracy \n",
    "    losses = np.zeros((int(num_epochs*np.ceil(A_train.shape[0] / batch_size))))\n",
    "    accs = np.zeros(losses.shape)\n",
    "    noncvx_losses = np.zeros(losses.shape)\n",
    "    \n",
    "    losses_test = np.zeros((num_epochs+1))\n",
    "    accs_test = np.zeros((num_epochs+1))\n",
    "    noncvx_losses_test = np.zeros((num_epochs+1))\n",
    "    \n",
    "    times = np.zeros((losses.shape[0]+1))\n",
    "    times[0] = time.time()\n",
    "    \n",
    "    # dataset loaders (minibatch)\n",
    "    ds = PrepareData(X=A_train, y=y_train)\n",
    "    ds = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    ds_test = PrepareData(X=A_test, y=y_test)\n",
    "    ds_test = DataLoader(ds_test, batch_size=A_test.shape[0], shuffle=False) # note batch_size\n",
    "    \n",
    "    losses_test[0], accs_test[0], noncvx_losses_test[0] = validation_cvxproblem(model, ds_test, beta, rho) # loss on the entire test set\n",
    "    \n",
    "    iter_no = 0\n",
    "    for i in range(num_epochs):\n",
    "        for ix, (_x, _y) in enumerate(ds):\n",
    "            #=========make input differentiable=======================\n",
    "            _x = Variable(_x).float()\n",
    "            _y = Variable(_y).float()\n",
    "\n",
    "            \n",
    "            ####### this function is for LBFGS ######\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                yhat = model(_x).float()\n",
    "                loss = loss_func_cvxproblem(yhat, _y, model, _x, beta, rho)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            #########################################\n",
    "            \n",
    "            \n",
    "            \n",
    "            #========forward pass=====================================\n",
    "            yhat = model(_x).float()\n",
    "            \n",
    "            loss = loss_func_cvxproblem(yhat, _y, model, _x, beta, rho)\n",
    "            correct = torch.eq(torch.sign(yhat), _y).float().sum() # accuracy\n",
    "\n",
    "            #=======backward pass=====================================\n",
    "            if solver_type == \"sgd\":\n",
    "                optimizer.zero_grad() # zero the gradients on each pass before the update\n",
    "                loss.backward() # backpropagate the loss through the model\n",
    "                optimizer.step() # update the gradients w.r.t the loss\n",
    "            elif solver_type == \"LBFGS\":\n",
    "                optimizer.step(closure)\n",
    "                \n",
    "\n",
    "            losses[iter_no] = loss.item() # loss on the minibatch\n",
    "            accs[iter_no] = correct\n",
    "            noncvx_losses[iter_no] = get_nonconvex_cost(_y, model, _x, beta)\n",
    "        \n",
    "            iter_no += 1\n",
    "            times[iter_no] = time.time()\n",
    "        \n",
    "        # get test loss and accuracy\n",
    "        losses_test[i+1], accs_test[i+1], noncvx_losses_test[i+1] = validation_cvxproblem(model, ds_test, beta, rho) # loss on the entire test set\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            print(\"Epoch [{}/{}], TRAIN: noncvx/cvx loss: {}, {} acc: {}. TEST: noncvx/cvx loss: {}, {} acc: {}\".format(i, num_epochs,\n",
    "                    np.round(noncvx_losses[iter_no-1], 3), np.round(losses[iter_no-1], 3), np.round(accs[iter_no-1]/batch_size, 3), \n",
    "                    np.round(noncvx_losses_test[i+1], 3), np.round(losses_test[i+1], 3), np.round(accs_test[i+1]/A_test.shape[0], 3)))\n",
    "            \n",
    "    return noncvx_losses, accs/batch_size, noncvx_losses_test, accs_test/A_test.shape[0], times, model, losses, losses_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "executionInfo": {
     "elapsed": 645,
     "status": "error",
     "timestamp": 1618022709516,
     "user": {
      "displayName": "Elijah Spiegel",
      "photoUrl": "",
      "userId": "13789567813973881906"
     },
     "user_tz": 420
    },
    "id": "GtZ8ugDZimPJ",
    "outputId": "9bf4bb66-f1fb-4daa-b6dd-8ef3cd78b108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 3072) (49000, 10) (1000, 3072) (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "# cifar-10 -- using the version downloaded from \"http://www.cs.toronto.edu/~kriz/cifar.html\"\n",
    "# A=sio.loadmat('CIFAR10_multiclass.mat')['A']\n",
    "# A_test=sio.loadmat('CIFAR10_multiclass.mat')['Atest']\n",
    "# y=sio.loadmat('CIFAR10_multiclass.mat')['y1']\n",
    "# y_test=sio.loadmat('CIFAR10_multiclass.mat')['ytest1']\n",
    "\n",
    "A = data['X_train']\n",
    "A = A.reshape((A.shape[0], -1))\n",
    "A_test = data['X_val']\n",
    "A_test = A_test.reshape((A_test.shape[0], -1))\n",
    "y_ints = data['y_train']  # changed y to y_ints\n",
    "y_test_ints = data['y_val']  # changed y_test to y_test_ints\n",
    "\n",
    "\"\"\" Our changes \"\"\"\n",
    "y = np.zeros((y_ints.shape[0], 10))\n",
    "y[np.arange(y.shape[0]), y_ints] = 1\n",
    "y_test = np.zeros((y_test_ints.size, 10))\n",
    "y_test[np.arange(y_test_ints.size), y_test_ints] = 1\n",
    "\n",
    "meanA=np.mean(A,axis=0)\n",
    "A=A-meanA\n",
    "A_test=A_test-meanA\n",
    "\n",
    "\"\"\" Tolga's code\n",
    "# to get the first two classes only (for binary classification)\n",
    "inds = np.argwhere(y <= 1)[:,0] # get the classes 0 and 1\n",
    "A = A[inds, :]\n",
    "y = y[inds].reshape((inds.shape[0], 1))\n",
    "y=2*y.astype(float)-1\n",
    "inds_test = np.argwhere(y_test <= 1)[:,0] # get the classes 0 and 1\n",
    "A_test = A_test[inds_test, :]\n",
    "y_test = y_test[inds_test].reshape((inds_test.shape[0], 1))\n",
    "y_test=2*y_test.astype(float)-1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "n, d = A.shape\n",
    "print(A.shape, y.shape, A_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [],
    "id": "Yofxh-IiimPL",
    "outputId": "348f9bab-e8c7-468f-cf70-25f3cb71cbd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sign patterns generated: 50\n",
      "x shape torch.Size([1000, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([1000, 50])\n",
      "yhat shape torch.Size([1000, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n",
      "x shape torch.Size([200, 3072])\n",
      "v shape torch.Size([3072, 50])\n",
      "w shape torch.Size([3072, 50])\n",
      "h shape torch.Size([3072, 50])\n",
      "raw DXv_w shape torch.Size([200, 50])\n",
      "yhat shape torch.Size([200, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-18a6bec69fe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;31m#5e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m results_cvx = sgd_solver_cvxproblem(A, y, A_test, y_test, num_epochs, num_neurons, beta, \n\u001b[0;32m---> 13\u001b[0;31m                         learning_rate, batch_size, rho, u_vector_list, solver_type, LBFGS_param, verbose)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-3a768194e838>\u001b[0m in \u001b[0;36msgd_solver_cvxproblem\u001b[0;34m(A_train, y_train, A_test, y_test, num_epochs, num_neurons, beta, learning_rate, batch_size, rho, u_vector_list, solver_type, LBFGS_param, verbose)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msolver_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# zero the gradients on each pass before the update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backpropagate the loss through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update the gradients w.r.t the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msolver_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LBFGS\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/convexlayerwiselearning/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/convexlayerwiselearning/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SGD solver for the nonconvex problem\n",
    "# generate random sign patterns\n",
    "P, verbose = 50, True # P is number of sign patterns to generate *and* number of neurons\n",
    "num_epochs, batch_size = 50, 200\n",
    "\n",
    "num_neurons, sign_pattern_list, u_vector_list = generate_sign_patterns(A, P, verbose)\n",
    "beta=1e-3\n",
    "rho = 1e-5\n",
    "solver_type = \"sgd\" # pick: \"sgd\" or \"LBFGS\"\n",
    "LBFGS_param = [10, 4] # these parameters are for the LBFGS solver\n",
    "learning_rate = 1e-7#5e-6\n",
    "results_cvx = sgd_solver_cvxproblem(A, y, A_test, y_test, num_epochs, num_neurons, beta, \n",
    "                        learning_rate, batch_size, rho, u_vector_list, solver_type, LBFGS_param, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMkGyl70imPM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convexNN_py1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:convexlayerwiselearning]",
   "language": "python",
   "name": "conda-env-convexlayerwiselearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
